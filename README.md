# GestureRecognizer â€” Reconhecimento de Gestos em LIBRAS

ğŸŒ DisponÃ­vel em: [English](README.en.md) | [PortuguÃªs](README.md)

**LIBRAS** (LÃ­ngua Brasileira de Sinais) Ã© a lÃ­ngua oficial de sinais usada no Brasil, reconhecida por lei e amplamente adotada para a comunicaÃ§Ã£o pela comunidade surda.

Um protÃ³tipo compacto de ponta a ponta que **captura marcos da mÃ£o**, **treina um classificador** e **reconhece o alfabeto da LIBRAS em tempo real** atravÃ©s de uma interface desktop simples.

> **Status:** em desenvolvimento â€” atualmente reconhece **letras**; prÃ³ximos marcos incluem **gestos/sinais completos** alÃ©m do alfabeto.

---

## âœ¨ Funcionalidades
- **Captura** marcos da mÃ£o pela webcam (OpenCV + MediaPipe), salvando um **dataset em CSV** (x, y, z para 21 marcos).
- **Treina** um modelo **KNN** (scikit-learn) com acurÃ¡cia, relatÃ³rio de classificaÃ§Ã£o e matriz de confusÃ£o.
- **Reconhece** letras em tempo real, sobrepondo a previsÃ£o no vÃ­deo.
- **Interface Desktop (Tkinter)** com trÃªs aÃ§Ãµes: **Capturar**, **Treinar**, **Reconhecer**.

- **Comportamento de fallback**:
  - Se nenhum `database.csv` for encontrado, o treinamento usa automaticamente `examples/sample_database.csv`.
  - Se nenhum `model.pkl` for encontrado, o reconhecimento solicita que o usuÃ¡rio treine um modelo primeiro.

---

## ğŸ¬ DemonstraÃ§Ã£o

Confira o vÃ­deo completo da demonstraÃ§Ã£o no LinkedIn:

- [Assista Ã  demo aqui](https://www.linkedin.com/seu-video-aqui)

---

## ğŸ—‚ï¸ Estrutura do Projeto
    .
    â”œâ”€ main.py                   # Janela Tkinter que orquestra: Captura â†’ Treina â†’ Reconhece
    â”œâ”€ gesture_capture.py        # Abre a webcam, captura 21 marcos (x,y,z), adiciona linhas ao database.csv
    â”œâ”€ train_model.py            # Carrega CSV, treina KNN (scikit-learn), mostra mÃ©tricas, salva model.pkl
    â”œâ”€ gesture_recognition.py    # InferÃªncia ao vivo: desenha marcos e sobrepÃµe a letra prevista
    â”œâ”€ database.csv              # (auto-criado) Dataset de amostras rotuladas
    â”œâ”€ model.pkl                 # (auto-criado) Modelo KNN treinado
    â”œâ”€ examples/sample_database.csv  # Pequeno dataset de demonstraÃ§Ã£o (usado se nÃ£o houver database.csv)
    â””â”€ requirements.txt          # DependÃªncias fixadas para Python 3.10

### O que cada arquivo faz
- **main.py** â€” pequeno controlador (Tkinter) com 3 botÃµes que executam os scripts abaixo.
- **gesture_capture.py** â€” teclas: `s` salva amostra, `m` entra em multi-salvamento (define o rÃ³tulo uma vez, depois pressione `s` repetidamente), `l` sai do multi-salvamento, `q` sai.
- **train_model.py** â€” separa treino/teste, treina **KNN (k=3)**, imprime **acurÃ¡cia**, **relatÃ³rio de classificaÃ§Ã£o**, **matriz de confusÃ£o**, e salva **model.pkl**. Se nenhum `database.csv` for encontrado, usa `examples/sample_database.csv`.
- **gesture_recognition.py** â€” previsÃ£o em tempo real; se **probabilidade mÃ¡xima < 0.9**, mostra **desconhecido** para evitar palpites superconfiantes. Se nenhum `model.pkl` for encontrado, solicita ao usuÃ¡rio treinar o modelo primeiro.

---

## ğŸ§° Stack TecnolÃ³gico
- **Python 3.10** 
- **OpenCV**
- **MediaPipe Hands**
- **NumPy / Pandas**
- **scikit-learn (KNN)**
- **Joblib**
- **Tkinter**

---

## âš™ï¸ InstalaÃ§Ã£o (Python 3.10 + ambiente virtual)

O MediaPipe nem sempre suporta as versÃµes mais recentes do Python. Use **Python 3.10** em um ambiente virtual para evitar problemas.

### 1) Instalar Python 3.10
- baixe a versÃ£o 3.10.x de python.org (marque â€œAdd Python to PATHâ€ durante a instalaÃ§Ã£o).

### 2) Criar e ativar um ambiente virtual

**Windows (PowerShell)**
    py -3.10 -m venv .venv
    .\.venv\Scripts\Activate.ps1
    python -m pip install --upgrade pip

**macOS / Linux**
    python3.10 -m venv .venv
    source .venv/bin/activate
    python -m pip install --upgrade pip

### 3) Instalar dependÃªncias

Com o arquivo requirements.txt jÃ¡ incluso no projeto, instale tudo executando:
    pip install -r requirements.txt

### 4) Rodar o app
    python main.py

---

### InÃ­cio rÃ¡pido com dataset de demonstraÃ§Ã£o
Se quiser apenas testar sem capturar novos gestos:
1. Execute `train_model.py` â†’ usarÃ¡ `examples/sample_database.csv` se nÃ£o existir `database.csv`.
2. Execute `gesture_recognition.py` â†’ pedirÃ¡ `model.pkl`; se nÃ£o existir, `train_model.py` deve ser executado primeiro.

---

**Dicas**
- Se a cÃ¢mera no Ã­ndice 0 falhar, altere `cv2.VideoCapture(0)` para `cv2.VideoCapture(1)` (ou 2, â€¦).
- Na captura: `s` salva, `m` habilita multi-salvamento (define rÃ³tulo uma vez), `l` sai do multi-salvamento, `q` fecha.

---

## ğŸ§­ Roadmap
- [ ] Adicionar reconhecimento de **gestos/sinais completos** (alÃ©m das letras)
- [ ] Melhorar ferramentas de dataset (balanceamento, aumento de dados)
- [ ] Exportar/importar datasets e modelos via interface

---

## ğŸ¤ Agradecimentos
- [MediaPipe Hands](https://developers.google.com/mediapipe)
- [scikit-learn](https://scikit-learn.org/)
